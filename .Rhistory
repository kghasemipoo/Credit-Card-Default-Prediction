install.packages(c("tidyverse", "readr", "dplyr", "ggplot2", "caret", "e1071", "randomForest"))
library(tidyverse)     # for data manipulation and plotting
library(readr)         # for reading CSV files
library(dplyr)         # for working with data frames
library(ggplot2)       # for data visualization
library(caret)         # for ML training and evaluation
library(e1071)         # for SVM and helper functions
library(randomForest)  # for Random Forest model
# Load the data
df <- read_csv("data.csv")
# Preview the first few rows
head(df)
# Get a summary of structure
glimpse(df)
# Rename the target variable to 'default'
df <- df %>%
rename(default = `default.payment.next.month`)
View(df)
df <- df %>%
mutate(
SEX = factor(SEX, levels = c(1, 2), labels = c("Male", "Female")),
EDUCATION = factor(EDUCATION),
MARRIAGE = factor(MARRIAGE),
default = factor(default, levels = c(0, 1), labels = c("No", "Yes"))
)
df <- df %>%
select(-...1)
# Get a summary of structure
glimpse(df)
# Count how many customers defaulted vs not
table(df$default)
# Plot the distribution
ggplot(df, aes(x = default, fill = default)) +
geom_bar() +
labs(title = "Default Status Distribution", x = "Default", y = "Number of Customers") +
theme_minimal()
table(df$default)
ggplot(df, aes(x = default, y = LIMIT_BAL, fill = default)) +
geom_boxplot() +
labs(title = "Credit Limit vs Default", y = "Credit Limit", x = "Default Status") +
theme_minimal()
ggplot(df, aes(x = default, y = AGE, fill = default)) +
geom_boxplot() +
labs(title = "Age vs Default", y = "Age", x = "Default Status") +
theme_minimal()
ggplot(df, aes(x = as.factor(PAY_0), fill = default)) +
geom_bar(position = "fill") +
labs(title = "PAY_0 (Recent Delay) vs Default", x = "PAY_0 (Repayment Status)", y = "Proportion") +
theme_minimal()
set.seed(123)  # for reproducibility
# Split into training (80%) and testing (20%)
split <- createDataPartition(df$default, p = 0.8, list = FALSE)
train_data <- df[split, ]
test_data <- df[-split, ]
# Train logistic regression model
log_model <- glm(default ~ ., data = train_data, family = "binomial")
# View summary
summary(log_model)
# Predict default probabilities
log_probs <- predict(log_model, newdata = test_data, type = "response")
# Convert probabilities to class labels using threshold 0.5
log_preds <- ifelse(log_probs > 0.5, "Yes", "No")
# Convert to factor to match actual labels
log_preds <- factor(log_preds, levels = c("No", "Yes"))
# Evaluate predictions
confusionMatrix(log_preds, test_data$default)
# Set up training control with upsampling
ctrl_up <- trainControl(
method = "cv",                   # cross-validation
number = 5,                      # 5-fold CV
sampling = "up",                 # upsampling
classProbs = TRUE,               # needed for ROC later
summaryFunction = twoClassSummary # better metrics
)
# Train logistic regression with upsampling
log_model_up <- train(
default ~ .,
data = train_data,
method = "glm",
family = "binomial",
trControl = ctrl_up,
metric = "ROC"  # use AUC-ROC as performance metric
)
# Predict class labels on test data
log_preds_up <- predict(log_model_up, newdata = test_data)
# Confusion matrix
confusionMatrix(log_preds_up, test_data$default)
ctrl_rf <- trainControl(
method = "cv",
number = 5,
sampling = "up",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# Train Random Forest with caret
rf_model <- train(
default ~ .,
data = train_data,
method = "rf",
trControl = ctrl_rf,
metric = "ROC",     # use ROC-AUC for performance
ntree = 100         # number of trees (optional, can tune later)
)
# Predict on test data
rf_preds <- predict(rf_model, newdata = test_data)
# Confusion matrix
confusionMatrix(rf_preds, test_data$default)
varImpPlot(rf_model$finalModel, main = "Feature Importance - Random Forest")
# Get importance from caret model
importance_df <- varImp(rf_model, scale = TRUE)
# Plot using ggplot2
ggplot(importance_df, aes(x = reorder(rownames(importance_df$importance), Overall), y = Overall)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(
title = "Feature Importance - Random Forest",
x = "Feature",
y = "Importance"
) +
theme_minimal()
library(pROC)
# Predict probabilities
log_probs <- predict(log_model, newdata = test_data, type = "response")
rf_probs  <- predict(rf_model, newdata = test_data, type = "prob")[, "Yes"]
# True labels: 1 = Yes (default), 0 = No
actual <- ifelse(test_data$default == "Yes", 1, 0)
# ROC curves
roc_log <- roc(actual, log_probs)
roc_rf  <- roc(actual, rf_probs)
# Plot ROC
plot(roc_log, col = "blue", main = "ROC Curve Comparison", legacy.axes = TRUE)
lines(roc_rf, col = "green")
legend("bottomright", legend = c("Logistic", "Random Forest"), col = c("blue", "green"), lwd = 2)
library(pROC)
# Predict probabilities
log_probs <- predict(log_model, newdata = test_data, type = "response")
rf_probs  <- predict(rf_model, newdata = test_data, type = "prob")[, "Yes"]
# True labels: 1 = Yes (default), 0 = No
actual <- ifelse(test_data$default == "Yes", 1, 0)
# ROC curves
roc_log <- roc(actual, log_probs)
roc_rf  <- roc(actual, rf_probs)
# Plot ROC
plot(roc_log, col = "blue", main = "ROC Curve Comparison", legacy.axes = TRUE)
lines(roc_rf, col = "red")
legend("bottomright", legend = c("Logistic", "Random Forest"), col = c("blue", "red"), lwd = 2)
auc(roc_log)
auc(roc_rf)
# Plot ROC
plot(roc_log, col = "blue", main = "ROC Curve Comparison", legacy.axes = TRUE)
lines(roc_rf, col = "red")
legend("bottomright", legend = c("Logistic", "Random Forest"), lwd = 2)
# Plot ROC
plot(roc_log, , main = "ROC Curve Comparison", legacy.axes = TRUE)
lines(roc_rf,
legend("bottomright", legend = c("Logistic", "Random Forest"), col = c("blue", "red"), lwd = 2)
